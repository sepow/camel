% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{The Kruskal-Wallis Test}\label{chap:kruskal}
\setcounter{page}{1}
\startcontents[chapters]
%----------------------------------------------------------------------
\dictum{X}{X}{X}
\chapcontents

%====================================================================
\section{The $\chi^2_n$ distribution}
%====================================================================

% defn: chi-squared distribution
\begin{definition}\label{defn:chisquared_dist}
Let $Z_1,Z_2,\ldots,Z_n$ be independent $ N(0,1)$ random variables. The distribution of $T = \sum_{i=1}^n Z_i^2$ is called the \emph{chi-squared distribution with $n$ degrees of freedom}, and has PDF
\[
f(t) = \frac{\left(\frac{1}{2}\right)^{n/2}t^{n/2-1}e^{-t/2}}{\Gamma(n/2)}, \quad t>0.
\]
\end{definition}

\bit
\it $\expe(T)=n$ and $\var(T)=2n$.
\it This is a special case of the gamma distribution.
\eit

%--------------------------------------------------
\subsection{The $\chi^2_n(\lambda)$ distribution}
%--------------------------------------------------
\begin{definition}
Let $X_1,X_2,\ldots,X_n$ be independent random variables with $X_i\sim N(\mu_i,1)$. The distribution of $W=\sum_{i=1}^n X_i^2$ is called the \emph{non-central chi-squared distribution}, with non-centrality parameter $\lambda = \sum_{i=1}^n \mu_i^2$. 
\end{definition}

\bit
%\it The case $\lambda = 0$ yields the (central) chi-squared distribution.
\it $\expe(W) = n+\lambda$ and $\var(W)=2(n+2\lambda)$. 
\eit

%--------------------------------------------------
\subsubsection*{Variance estimation}
%--------------------------------------------------
Let $X_1,X_2,\ldots,X_n$ be a random sample from the $N(\mu,\sigma^2)$-distribution. Under $H_0:\sigma^2=\sigma_0^2$,
\ben
\it If $\mu$ is known, $\qquad T = \displaystyle\frac{1}{\sigma_0^2}\sum_{i=1}^n (X_i-\mu)^2 \sim \chi^2_n$.
\it If $\mu$ is unknown, $\quad T = \displaystyle\frac{1}{\sigma_0^2}\sum_{i=1}^n (X_i-\bar{X})^2 \sim \chi^2_{n-1}$.
\een

%--------------------------------------------------
\subsection{The $F_{m,n}$ distribution}
%--------------------------------------------------
\begin{definition}
Let $X$ and $Y$ be independent random variables with $X\sim\chi^2_m$ and $Y\sim\chi^2_n$. The \emph{$F$ distribution with $m$ and $n$ degrees of freedom} is the distribution of the ratio
\[
%T = \frac{\displaystyle\frac{1}{m}X}{\displaystyle\frac{1}{n}Y}.
F = \frac{X/m}{Y/n}.
\]
\end{definition}

%\begin{remark}
\ben
\it Tables usually only give upper-tail percentage points. 
\it Where necessary, we can use the fact that if $F\sim F_{m,n}$, then $1/F\sim F_{n,m}$.
%If $T\sim F_{m,n}$ and we want to find $t$ such that $\prob(T<t)=\alpha$, we can use $F_{n,m}$ tables to find $1/t$ such that $\prob(1/T>1/t)=\alpha$.
%\it If $X$ has Student's $t$ distribution with $n$ degrees of freedom, then $X^2\sim F_{1,n}$.
\een
%\end{remark}

%--------------------------------------------------
\subsubsection*{Comparison of variances}
%--------------------------------------------------
Let $X_1,X_2,\ldots,X_m$ and $Y_1,Y_2,\ldots,Y_n$ be two independent random samples, from the $N(\mu_1,\sigma_1^2)$ and $N(\mu_2,\sigma_2^2)$ distributions respectively (where $\mu_1$ and $\mu_2$ are unknown).

\bit
\it Consider the null hypothesis $H_0:\sigma_1^2 = \sigma_2^2$.
\eit

The ratio of the sample variances provides a useful test statistic:
\[
F	\ = \ \frac{s_1^2}{s_2^2}
	\ = \ \frac{\displaystyle\frac{1}{m-1} \sum_{i=1}^m (X_i-\bar{X})^2}{\displaystyle\frac{1}{n-1}\sum_{j=1}^n (Y_j-\bar{Y})^2} 
	\ = \ \frac{\sigma_1\left[\displaystyle\frac{1}{m-1}\sum_{i=1}^m \left(\frac{X_i-\bar{X}}{\sigma_1}\right)^2\right]
			}{\sigma_2\left[\displaystyle\frac{1}{n-1}\sum_{j=1}^n \left(\frac{Y_j-\bar{Y}}{\sigma_2}\right)^2\right]} .
%	\ = \ \left(\frac{\sigma_1^2}{\sigma_2^2}\right) F_{m-1,n-1}.
\]

Under $H_0:\sigma_1^2=\sigma_2^2$, this ratio has the $F_{m-1,n-1}$ distribution.

%====================================================================

\section{One-way ANOVA}
%====================================================================
ANOVA is a way of testing hypotheses about means by looking at sample variances.

\vspace*{2ex}
Statistical model:
\[
X_{ij} = \mu_i + \epsilon_{ij},\quad j=1,2,\ldots,n_i,\quad i=1,2,\ldots,k.
\]
where the $\epsilon_{ij}\sim N(0,\sigma^2)$ are independent.

\vspace*{2ex}
\bit
\it There are $k$ groups (indexed by $i=1,2,\ldots,k$).
\it The are $n_i$ observations in the $i$th group (indexed by $j=1,2,\ldots,n_i$).
\it Assume that the observations $X_{ij}\sim N(\mu_i,\sigma^2)$ are independent.
\eit

\vspace*{2ex}
Hypothesis test:
\begin{align*}
& H_0:\ \mu_1=\mu_2=\ldots=\mu_k, \\
& H_1:\ \mu_i\neq\mu_j \text{ for some } i\neq j.
\end{align*}
%\[
%H_0:\mu_1=\mu_2=\ldots=\mu_k \text{\quad against\quad} H_1: \mu_i\neq\mu_j \text{ for some } i\neq j.
%\]

Let $N=\sum_{i=1}^k n_i$ be the total number of observations. 
\begin{align*}
\text{Group means:}\qquad & \bar{X}_{i\cdot} 	= \frac{1}{n_i}\sum_{j=1}^{n_i} X_{ij} \text{\quad for $i=1,2,\ldots k$.} \\
\text{Grand mean:}\qquad	& \bar{X}_{\cdot\cdot} 	= \frac{1}{N}\sum_{i=1}^k\sum_{j=1}^{n_i} X_{ij} 
\end{align*}

%\bit
%\it The $\bar{X}_{i\cdot}$ are called the \emph{group means} ($i=1,2,\ldots,k$).
%\it $\bar{X}_{\cdot\cdot}$ is called the \emph{overall mean} or \emph{grand mean}.
%\eit

The total deviation of a single observation from the overall mean can be divided into two components:
\[
\begin{array}{ccccc}
(X_{ij}-\bar{X}_{\cdot\cdot})& = & (X_{ij}-\bar{X}_{i\cdot})	& + & (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})  \\
\text{Total deviation} 		& & \text{Within-group deviation}			& & \text{Between-groups deviation} 	\\
							& & \text{(unexplained)}		& & \text{(explained)} 		
\end{array}
\]
\ben
\it $X_{ij}-\bar{X}_{\cdot\cdot}$ is the deviation of an individual observation from the overall mean.
\it $X_{ij}-\bar{X}_{i\cdot}$ is the deviation of an individual observation from its group mean.
\it $\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot}$ is the deviation of a group mean from the overall mean.
\een

%Under $H_0$, we have independent estimates of $\sigma^2$:
%\bit
%\it Between groups: $\frac{1}{k-1}\sum_{i=1}^{k}(\bar{X}_{i\cdot} - \bar{X}_{..})^2$
%\it Within groups: $\frac{1}{n_i-1}\sum_{j=1}^{n_i}(X_{ij} - \bar{X}_{i\cdot})^2$
%\eit
%



We define the following sums-of-squares:
\[
\begin{array}{lll}
S_T	& = \displaystyle\sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{\cdot\cdot})^2
\qquad & \text{the \textbf{total} sum-of-squares,} \\
S_G	& = \displaystyle\sum_{i=1}^k n_i (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2
\qquad & \text{the \textbf{between-groups} sum-of-squares,} \\
S_E	& = \displaystyle\sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2
\qquad & \text{the \textbf{within-groups} sum-of-squares.} \\
\end{array}
\]

\begin{lemma}
$S_T = S_G + S_E$.
\end{lemma}
\vspace*{-2ex}
\begin{proof}
Exercise.
%\begin{align*}
%S_T
%	& = \sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{\cdot\cdot})^2 \\
%	& = \sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot}+\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2 \\
%	& = \sum_{i=1}^k\sum_{j=1}^{n_i} \big[(X_{ij}-\bar{X}_{i\cdot})^2 + 2(X_{ij}-\bar{X}_{i\cdot})(\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})+ (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2\big] \\
%	& = \sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2 + \sum_{i=1}^k n_i(\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2 \\
%	& = S_E + S_G.
%\end{align*}
\end{proof}
\vspace*{-2ex}
\bit
\it $S_T$ is the total variability in the data (total squared deviation),
\it $S_G$ is the variability explained by group membership,
\it $S_E$ is the residual (or unexplained) variability.
%\it[]
%\it Intuitively, if the ratio $S_E/S_G$ is large we would be inclined to reject $H_0:\mu_1=\mu_2=\ldots=\mu_k$.
\eit


%The total variability ($S_0$) is partitioned into two parts: variability explained by group membership ($S_2$) and the residual variability, not explained by group membership ($S_1$).

\subsection{The distribution of $S_G = \sum_{i=1}^k n_i (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2$}
%\textbf{Distribution of $S_G = \sum_{i=1}^k n_i (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2$}:\par

Between groups, we have $\{n_i\bar{X}_{i\cdot}:i=1,2,\ldots,k\}$ independent with $n_i\bar{X}_{i\cdot}\sim N(n_i\mu_i,n_i\sigma^2)$. Hence
\[
\frac{1}{\sigma^2}\sum_{i=1}^k n_i (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2 \sim \chi^2_{k-1}(\lambda)
\quad\text{where}\quad\lambda = \sum_{i=1}^k n_i(\mu_i-\bar{\mu})^2
\text{ and } \bar{\mu}=\frac{1}{N}\sum_{i=1}^k n_i\mu_i.
\]
%where $\lambda = \sum_{i=1}^k n_i(\mu_i-\bar{\mu})$ and $\bar{\mu}=\frac{1}{N}\sum_{i=1}^k n_i\mu_i$ .


\bit
%\it $S_2\sim \sigma^2\chi^2_{k-1}(\lambda)$ where $\lambda = \sum_{i=1}^k n_i(\mu_i-\bar{\mu})$ and $\bar{\mu}=\frac{1}{N}\sum_{i=1}^k n_i\mu_i$ .
\it Under $H_0$ we have $\lambda=0$ and hence $\displaystyle\frac{1}{\sigma^2}S_G\sim \chi^2_{k-1}$.
\it Under $H_1$ we have $\lambda>0$, and $S_G$ tends to be inflated.
\eit

\subsection{The distribution of $S_E = \sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2$}
%\textbf{Distribution of $S_E = \sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2$}:\par

Within each group we have $\{X_{ij}:j=1,2,\ldots,n_i\}$ independent with $X_{ij}\sim N(\mu_i,\sigma^2)$, so
%\bit
%\it $\displaystyle\sum_{j=1}^{n_i}(X_{ij}-\bar{X}_{i\cdot})^2 \sim \sigma^2\chi^2_{n_i-1}$, so $S_1 \sim \sigma^2\chi^2_{N-k}$.
%\eit
\[
\frac{1}{\sigma^2}\sum_{j=1}^{n_i}(X_{ij}-\bar{X}_{i\cdot})^2 \sim \chi^2_{n_i-1}.
\]
Hence $\displaystyle\frac{1}{\sigma^2}S_E \sim \chi^2_{N-k}$. (This holds regardless of whether or not $H_0$ is true.)
%\ben
%\it $\sum_{j=1}^{n_i}(X_{ij}-\bar{X}_{i\cdot})^2 \sim \sigma^2\chi^2_{n_i-1}$,
%\it $S_1 \sim \sigma^2\chi^2_{N-k}$.
%\it This holds regardless of whether or not $H_0$ is true.
%\een


Under the null hypothesis $H_0:\mu_1=\mu_2=\ldots=\mu_k$,
\[
\frac{1}{\sigma^2}S_G\sim \chi^2_{k-1} \text{\quad and\quad} \frac{1}{\sigma^2}S_E \sim \chi^2_{N-k}.
\]
Thus we define the test statistic
\[
F = \displaystyle\frac{S_G/(k-1)}{S_E/(N-k)}.
\]
%\[
%F = \frac{\frac{1}{k-1}S_G}{\frac{1}{N-k}S_E}
%\]
%\[
%F = \frac{\displaystyle\frac{1}{k-1}S_G}{\displaystyle\frac{1}{N-k}S_E}
%\]
\bit
%\it If $H_0$ is true, $F = \frac{\frac{1}{k-1}S_2}{\frac{1}{N-k}S_1} \sim F_{k-1,N-k}$.
\it If $H_0$ is true, $F \sim F_{k-1,N-k}$.
\it If $H_0$ is not true, $F$ tends to be larger.
\eit
\vspace*{2ex}
We reject $H_0:\mu_1=\mu_2=\ldots=\mu_k$ whenever $F > F_c^{\alpha}$, where $F_c^{\alpha}$ is the upper-tail critical value of the $F_{k-1,N-k}$ distribution at significance level $\alpha$. 

%Thus we reject $H_0$ whenever $F$ exceeds upper-tail critical values of the $F_{k-1,N-k}$ distribution.
% at significance level $\alpha$. 

%%--------------------------------------------------
%\subsection{Mean squares}
%%--------------------------------------------------
%A \emph{mean square} is the ratio of a sum of squares to its degrees of freedom:
%
%\ben
%\it The \emph{total} mean squared deviation is the overall sample variance of the $X_{ij}$:
%\[
%M_T = \frac{S_0}{N-1} = \frac{1}{N-1}\sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{\cdot\cdot})^2.
%\]
%\it The \emph{between-groups} mean squared deviation is the sample variance of the group means $\bar{X}_{i\cdot}$:
%\[
%M_G = \frac{S_2}{k-1} = \frac{1}{k-1}\sum_{i=1}^k n_i (\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2.
%\]
%\it The \emph{within-groups} mean squared deviation is the sum of the sample variances within each group:
%\[
%M_E = \frac{S_1}{N-k} 
%	= \frac{1}{N-k}\sum_{i=1}^k\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2
%%	= \sum_{i=1}^k\left[\frac{1}{n_i-1}\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{\cdot\cdot})^2\right]
%	= \sum_{i=1}^k s^2_i
%\quad\text{where}\quad s^2_i = \frac{1}{n_i-1}\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2. 
%\]
%%where $s^2_i = \displaystyle\frac{1}{n_i-1}\sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{\cdot\cdot})^2$. 
%This is sometimes called the \emph{mean squared error} (under $H_1$).
%\een
%
%\bit
%\it Note that the $F$-statistic is the ratio $M_2/M_1$.
%\eit
%
%The various statistics computed during a one-way analysis of variance are usually reported in tabular form:
%\begin{center}
%\begin{tabular}{|l|c|c|c|c|} \hline
%Source 			& \qquad df\qquad\mbox{}	& \qquad SS\qquad\mbox{}	& \qquad MS\qquad\mbox{}	& \qquad F\qquad\mbox{}	\\ \hline
%Between Groups	& $k-1$					& $S_2$					& $M_2 = S_2/(k-1)$		& $F = M_2/M_1$			\\ \hline
%Within Groups 	& $N-k$					& $S_1$					& $M_1 = S_1/(N-k)$		&						\\ \hline
%Total			& $N-1$					& $S_0$					& 						& 						\\ \hline
%\end{tabular}\par
%\end{center}


%Assumptions:
%\bit
%\it All observations are independent
%\it All observations within a group have the same distribution.
%\it All distributions have the same form (i.e.\ they belong to a location family)
%\it The distributions are all \textbf{approximately normal}
%\eit

%====================================================================
%
%\section{The Kruskal-Wallis test}
%====================================================================
%Under $H_0$, we haave two independent estimates of $\sigma^2$:
%\bit
%\it Between groups: $\sum_{i=1}^{k}(\bar{X}_{i\cdot} - \bar{X}_{..})^2$
%\it Within groups: $\sum_{j=1}^{k}(X_{ij} - \bar{X}_{i\cdot}^2$
%\eit

%By the CLT, 
%\[
%\bar{R}_{\cdot\cdot} \sim N
%\]


%-----------------------------
% example

\begin{example}
The data below are the yields (per hectare) of eight types of wheat, recorded over four independent trials. 
\[
\begin{array}{|c|cccc|}\hline
\text{Type}	& \multicolumn{4}{c|}{\text{Yield}} \\ \hline
1 &  182 & 214 & 216 & 231 \\
2 &  196 & 202 & 208 & 224 \\
3 &  203 & 212 & 221 & 242 \\
4 &  198 & 203 & 207 & 222 \\
5 &  171 & 192 & 197 & 204 \\
6 &  194 & 218 & 223 & 232 \\
7 &  208 & 216 & 218 & 239 \\
8 &  183 & 188 & 193 & 198 \\ \hline
\end{array}
\]
Perform an analysis-of-variance to determine whether there are significant differences among the mean yields of the eight types.
\end{example}

\begin{solution}
\[
\begin{array}{|c|cccc|r|r|}\hline
\text{Type} (i)	& \multicolumn{4}{c|}{\text{Yield}} & \sum_j X_{ij} &  \sum_j X_{ij}^2 \\ \hline
1 				&  182 & 214 & 216 & 231 	&  843 &  178937 \\
2 				&  196 & 202 & 208 & 224 	&  830 &  172660 \\
3 				&  203 & 212 & 221 & 242 	&  878 &  193558 \\
4 				&  198 & 203 & 207 & 222 	&  830 &  172546 \\
5 				&  171 & 192 & 197 & 204 	&  764 &  146530 \\
6 				&  194 & 218 & 223 & 232 	&  867 &  188713 \\
7 				&  208 & 216 & 218 & 239 	&  881 &  194565 \\
8 				&  183 & 188 & 193 & 198 	&  762 &  145286 \\ \hline
\text{Overall}	&      &     &     &    		& 6655 & 1392795 \\ \hline
\end{array}
\]
\end{solution}

The sums of squares are
\[
S_T = 8762.9688,\quad S_G = 3848.71875 \text{\quad and\quad} S_E = 4914.2.
\]
%

%The sums of squares are computed as follows:
%\begin{align*}
%S_T
%	& = \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{\cdot\cdot})^2 \\
%	& = \sum_{i=1}^k \sum_{j=1}^{n_i} X_{ij}^2 - \frac{1}{N}\left(\sum_{i=1}^k\sum_{j=1}^{n_i} X_{ij}\right)^2 \\
%	& = (182^2 + 214^2 + \ldots + 762^2) - \frac{6655^2}{32} \\
%	& = 1392795 - 1384032.03125 = 8762.96875 \\ 
%\end{align*}
%
%\begin{align*}
%S_G
%	& = \sum_{i=1}^k n_i(\bar{X}_{i\cdot}-\bar{X}_{\cdot\cdot})^2 \\
%	& = \sum_{i=1}^k \frac{1}{n_i}\left(\sum_{j=1}^{n_i} X_{ij}\right)^2 
%			- \frac{1}{N}\left(\sum_{i=1}^k\sum_{j=1}^{n_i} X_{ij}\right)^2 \\
%	& \left(\frac{843^2}{4} + \frac{830^2}{4} +\ldots+ \frac{762^2}{4}\right) - \frac{6655^2}{32} \\
%	& = 1387880.75 - 1384032.03125 = 3848.71875. 
%\end{align*}
%\begin{align*}
%S_E
%	& = \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij}-\bar{X}_{i\cdot})^2  \\
%	& = \sum_{i=1}^k \sum_{j=1}^{n_i} X^2_{ij} - \sum_{i=1}^k \frac{1}{n_i}\left(\sum_{j=1}^{n_i} X_{ij}\right)^2 \\ 
%	& = (182^2 + 214^2 + \ldots + 762^2) - \left(\frac{843^2}{4} + \frac{830^2}{4} +\ldots+ \frac{762^2}{4}\right) \\
%	& = 1392795 - 1387880.75 = 4914.25 \\
%\end{align*}
%Check: $S_G + S_E = 3848.77 + 4914.20 = 8762.9 = S_0$.



The ANOVA table is:
\begin{center}
\begin{tabular}{|l|c|c|c|c|} \hline
Source		& df		& SS			& MS			& F 			\\ \hline
Between-Groups		& 7		& 3848.72	& 549.8170	& 2.6852		\\ \hline
Within-Groups		& 24		& 4914.25	& 204.7604 	& 			\\ \hline
Total		& 31		& 8762.97 	& 			&			\\ \hline
\end{tabular}\par
\end{center}

From tables of the $F_{7,24}$ distribution, 
\bit
\it The 95th percentile is $F_c^{0.05} = 2.42$.
\it The 99th percentile is $F_c^{0.01} = 3.50$.
\eit
The $p$-value of the test is between $0.01$ and $0.05$. On this evidence, the null hypothesis (of no difference in means) could be rejected, but the evidence is not particularly strong.

%====================================================================

\section{The Kruskal-Wallis Test}
%====================================================================

The \emph{Kruskal-Wallis test} is the non-parametric counterpart of one-way independent ANOVA,

\begin{itemize}
\item Applies one-way ANOVA to ranks, not the original observations.
\item Tests for the equality of medians across groups ($H_0:\eta_1=\eta_2=\ldots=\eta_k$).
\item An extension of the Mann-Whitney test to 3 or more groups.
%\item does not assume normal populations,
\item Assumes that population variances are equal across the different groups.
\end{itemize}


The Kruskal-Wallis test statistic is defined by
$$
H = \frac{12}{N(N+1)}\sum_{i=1}^k \frac{T_i^2}{n_i} - 3(N+1)
$$
where $T_i$ is the sum of ranks in the $i$th group:
\[
T_i = \sum_{j=1}^{n_i} R_{ij} \text{\quad where $R_{ij}$ is the rank of $X_{ij}$ in the pooled sample.}
\]
$H$ is approximately distributed according to the $\chi^2$-distribution with $k-1$ degrees of freedom.
%
%\[
%T_i = \sum_{j=1}^{n_i} R_{ij} \text{\quad where\quad} R_{ij} = \sum_{\alpha=1}^k\sum_{\beta=1}^{n_\alpha} I(X_{\alpha\beta}<X_{ij}).
%\]
%
%\begin{itemize}
%\item $k$ is the number of groups.
%\item $T_i$ is the sum of ranks in the $i$th group.
%\item $n_i$ is the number of observations in the $i$th group
%\item $N$ is the total number of sample points.
%\end{itemize}

%\begin{itemize}
%\item $H$ is approximately distributed according to the $\chi^2$-distribution with $k-1$ degrees of freedom.
%\end{itemize}

%============================= 

%\section{Example}
%============================= 
\begin{example}
A health administrator wants to compare the unoccupied bed space for three hospitals located in the same city. She randomly selects 10 different days from the records of each hospital and lists the number of unoccupied beds for each day.
\end{example}
\small
\begin{tabular}{l|cccccccccc|c}\hline
Day 		&  1 &  2 &  3 &  4 &  5 &  6 &  7 &  8 &  9 & 10 & Rank Sum \\ \hline
Hospital 1 	&  6 & 38 &  3 & 17 & 11 & 30 & 15 & 16 & 25 & 5  & 120   \\ 
Hospital 2 	& 34 & 28 & 42 & 13 & 40 & 31 &  9 & 32 & 39 & 27 & 210.5 \\ 
Hospital 3 	& 13 & 35 & 19 &  4 & 29 &  0 &  7 & 33 & 18 & 24 & 134.5 \\ \hline
\end{tabular}
\normalsize
\vspace*{4ex}

\begin{solution}
The Kruskal-Wallis test statistic is
\begin{align*}
H 	& = \frac{12}{30(31)}\left(\frac{(120)^2}{10} + \frac{(210.5)^2}{10} + \frac{(134.5)^2}{10}\right)-3(31)
%  	& = 99.097 - 93 \\
  	= 6.097.
\end{align*}


\bit
\it Because the groups are large, we can approximate the critical value at $\alpha=0.05$ by $\chi^2_{0.05}(2)=5.991$. 
\it Since $H>5.991$, we reject the null hypothesis and conclude that at least one of the three hospitals has more empty beds than at least one of the others.
\eit
\end{solution}


%============================= 
\subsection{The Kruskal-Wallis test statistic}
%============================= 
Let $R_{ij}$ be the rank of $X_{ij}$ in the pooled sample. The total deviation of $R_{ij}$ from the overall mean rank can be divided into two components:
\[
R_{ij} - \bar{R}_{\cdot\cdot} =  (R_{ij}-\bar{R}_{i\cdot}) + (\bar{R}_{i\cdot}-\bar{R}_{\cdot\cdot})
\]
\vspace*{-1ex}
where
\[
\bar{R}_{i\cdot} = \frac{1}{n_i}\sum_{j=1}^{n_i} R_{ij}
\text{\quad and\quad}
\bar{R}_{\cdot\cdot} = \frac{1}{N}\sum_{i=1}^k\sum_{j=1}^{n_i} R_{ij}.
\]
%
%As before, we can define
%\[
%\begin{array}{lll}
%S_T	& = \displaystyle\sum_{i=1}^k\sum_{j=1}^{n_i} (R_{ij}-\bar{R}_{\cdot\cdot})^2
%\qquad & \text{the \textbf{total} sum-of-squares,} \\
%S_G	& = \displaystyle\sum_{i=1}^k n_i (\bar{R}_{i\cdot}-\bar{R}_{\cdot\cdot})^2
%\qquad & \text{the \textbf{between-groups} sum-of-squares,} \\
%S_E	& = \displaystyle\sum_{i=1}^k\sum_{j=1}^{n_i} (R_{ij}-\bar{R}_{i\cdot})^2
%\qquad & \text{the \textbf{within-groups} sum-of-squares.} \\
%\end{array}
%\]
%with $S_T=S_G+S_E$.
%
%Then
%\begin{eqnarray*}
%\SST & = & \sum_{i=1}^k \sum_{j=1}^{n_i} (R_{ij}-\bar{R})^2 \\
%\SSM & = & \sum_{i=1}^k n_i(\bar{R}_i-\bar{R})^2
%\end{eqnarray*}

\vspace*{1ex}
Because we are dealing with ranks, the total sum of squared differences has a fixed value:
\vspace*{-3ex}
%\bit
%\it The overall mean rank is $\bar{R}_{\cdot\cdot}=\frac{1}{2}(N+1)$.
%%\it The total sum of squares is $S_G = \frac{1}{12}N(N-1)(N+1)$.
%\it The total sum of squares is $\displaystyle \sum_{i=1}^k\sum_{j=1}^{n_i} (R_{ij}-\bar{R}_{\cdot\cdot})^2 	= \frac{1}{12}N(N-1)(N+1)$.
%\eit

\begin{align*}
\sum_{i=1}^k\sum_{j=1}^{n_i} (R_{ij}-\bar{R}_{\cdot\cdot})^2
	& = \sum_{i=1}^k\sum_{j=1}^{n_i} R_{ij}^2 -\left(\frac{1}{N}\sum_{i=1}^k\sum_{j=1}^{n_i} R_{ij}\right)^2 \\
	& = \frac{1}{6}N(N+1)(2N+1) - \frac{1}{4}(N+1)^2 
	= \frac{1}{12}N(N^2-1).
\end{align*}

Thus we need only consider the so-called \textit{between-groups} sum of squares:
%squared differences $(R_{i\cdot}-\bar{R}_{\cdot\cdot})^2$ between the group means $R_{i\cdot}$ and the overall mean $\bar{R}_{\cdot\cdot}$.
\[
S_G = \sum_{i=1}^k n_i (\bar{R}_{i\cdot}-\bar{R}_{\cdot\cdot})^2
\]
%
%the sum $\sum_{i=1}^k\sum_{j=1}^{n_i} (R_{i\cdot}-\bar{R}_{\cdot\cdot})^2$ of squared differences can therefore be based only on us we need only consider the squared differences $\bar{R}_{\cdot\cdot}$ between group means and the overall mean.

%The test is therefore based only on the squared deviations between group means and the overall mean $\bar{R}_{\cdot\cdot}$.


%
%
%\vspace*{2ex}
%The test statistic can be written as
%\[
%H = \left(\frac{N-1}{N}\right)\sum_{i=1}^k \frac{n_i\big[\bar{R}_{i\cdot}-\frac{1}{2}(N+1)\big]^2}{\frac{1}{12}(N^2-1)}
%\]
%
%\vspace*{2ex}
%We can show that this is equivalent to the following:
%\[
%H = \left(\frac{N-1}{N}\right)\sum_{i=1}^k \left(\frac{\bar{R}_{i\cdot}-\expe(\bar{R}_{i\cdot})}{\sqrt{\var(\bar{R}_{i\cdot})}}\right)^2.
%\]
%\bit
%\it $H$ is a sum of squared deviations of standardized random variables.
%
%\it By the central limit theorem, $H\sim\chi^2_{k-1}$ approx. provided the $n_i$ are sufficiently large.
%\eit


% lemma: mean and variance of R_{ij}
\begin{lemma}
Under $H_0:\eta_1=\eta_2=\ldots=\eta_k$,
\[
\expe(R_{ij})=\frac{1}{2}(N+1) 
\text{\quad and\quad} 
\var(R_{ij}) = \frac{1}{12}(N^2-1).
\]
\end{lemma}
\begin{proof}
Under the null hypothesis, $R_{ij}\sim\text{Uniform}\{1,2,\ldots,N\}$, so
\begin{align*}
\expe(R_{ij})	
	& = \sum_{u=1}^N u\prob(R_{ij}=u) = \frac{1}{N}\sum_{u=1}^N u = \frac{1}{2}(N+1)\\
\intertext{and}
\expe(R^2_{ij})	
	& = \sum_{u=1}^N u^2\prob(R_{ij}=u) = \frac{1}{N}\sum_{u=1}^N u^2 = \frac{1}{6}(N+1)(2N+1) \\
\intertext{so}
\var(R_{ij})
	& = \frac{1}{6}(N+1)(2N+1) - \frac{1}{4}(N+1)^2 = \frac{1}{12}(N^2-1).
\end{align*}
\end{proof}

%% lemma: mean and variance of discrete uniform distribution
%\begin{lemma}
%If $Y\sim\text{Uniform}\{1,2,\ldots,N\}$ then
%$\expe(Y) = \frac{1}{2}(N+1)$ and $\var(Y) = \frac{1}{12}(N^2-1)$.
%\end{lemma}
%
%\begin{proof}
%Exercise.
%%Use $\sum_{j=1}^N j = \frac{1}{2}N(N+1)$ and $\sum_{j=1}^n j^2 = \frac{1}{6}N(N+1)(2N+1)$.
%\end{proof}
%
%Under $H_0$, $R_{ij}\sim\text{Uniform}\{1,2,\ldots,N\}$, so
%\[
%\expe(R_{ij}) = \frac{1}{2}{N+1}
%\text{\quad and\quad}
%\var{R_{ij}} = \frac{1}{12}{N^2-1}
%\]

%
%
%Consider the average rank of observations from group $i$:
%\[
%\bar{R}_{i\cdot} = \frac{1}{n_i}\sum_{j=1}^{n_i} R_{ij}.
%\]
%


% lemma: mean and variance of \bar{R}_{i\cdot}
\begin{lemma}
Under $H_0:\eta_1=\eta_2=\ldots=\eta_k$,
\[
\expe(\bar{R}_{i\cdot}) = \frac{1}{2}(N+1)
\text{\quad and\quad} 
\var(\bar{R}_{i\cdot}) = \frac{(N+1)(N-n_i)}{12n_i}.
\]
\end{lemma}

\begin{proof}
$\{R_{ij}:j=1,2,\ldots,n_i\}$ is a random sample chosen uniformly at random \emph{without replacement} from the finite population $\{1,2,\ldots,N\}$. If $X_1,X_2,\ldots,X_n$ is a random sample chosen uniformly at random without replacement from a finite population $\{x_1,x_2,\ldots,x_N\}$, then
\[
\expe(\bar{X}) = \mu \text{\quad and\quad} \var(\bar{X}) = \frac{\sigma^2}{n}\left(\frac{N-n}{N-1}\right),
\]
where $\mu$ and $\sigma^2$ are the common mean and variance of the $X_i$:
\[
\mu = \frac{1}{N}\sum_{j=1}^N x_j \text{\quad and\quad} \sigma^2 = \frac{1}{N}\sum_{j=1}^N (x_j-\mu)^2.
\]
The term $(N-n)/(N-1)$ is called the \emph{finite population correction} (FPC) factor.
\end{proof}
%Note that if $n_i=1$ then $\var(\bar{R}_{i\cdot})=\displaystyle\frac{1}{12}(N^2-1)$.
%\[
%H = \left(\frac{N-1}{N}\right)\sum_{i=1}^k \left(\frac{\bar{R}_{i\cdot}-\expe(\bar{R}_{i\cdot})}{\sqrt{\var(\bar{R}_{i\cdot})}}\right)^2.
%\]


By the central limit theorem, if $n_i$ is sufficiently large (say $n_i\geq 5$) then
\[
\frac{\bar{R}_{i\cdot} - \expe(\bar{R}_{i\cdot})}{\sqrt{\var(\bar{R}_{i\cdot})}} \sim N(0,1) \text{\quad approx.}
\]

By the previous lemma,
\[
\frac{\bar{R}_{i\cdot} - \frac{1}{2}(N+1)}{\sqrt{(N+1)(N-n_i)/12n_i}} \sim N(0,1) \text{\quad approx.}
\]
Hence, provided all $n_1,n_2,\ldots,n_k$ are all sufficiently large, 

\[
\sum_{i=1}^k \frac{\big[\bar{R}_{i\cdot} - \frac{1}{2}(N+1)\big]^2}{(N+1)(N-n_i)/12n_i} \sim \chi^2_{k-1}\text{\quad approx.}
\]
or equivalently
\[
\frac{12}{N+1}\sum_{i=1}^k \frac{n_i\big[\bar{R}_{i\cdot} - \frac{1}{2}(N+1)\big]^2}{(N+1)(N-n_i)} \sim \chi^2_{k-1}\text{\quad approx.}
\]
It can be shown that the LHS reduces to the Kruskal-Wallis test statistic $H$, defined earlier.
%The between-groups sum-of-squares is therefore
%
%\begin{align*}
%S_G 
%	& = \sum_{i=1}^k n_i (\bar{R}_{i\cdot}-\bar{R}_{\cdot\cdot})^2 \\
%	& = \sum_{i=1}^k n_i (\bar{R}_{i\cdot}-\bar{R}_{\cdot\cdot})^2 \\
%\end{align*}


%
%
%It is easy to show that
%$$
%\SSM = \sum_{i=1}^k n_i\bar{R}_i^2 - \frac{n(n+1)^2}{4}
%$$
%
%\begin{itemize}
%\item The Kruskal-Wallis statistic is defined by
%$$
%H = \frac{(n-1)\SSM}{\SST} = \frac{\SSM}{n(n+1)/12}
%$$
%\item
%Because $\SST$ is constant, $H$ is simply a sum of squares.
%\item
%$H$ is has approximately distributed according to a $\chi^2$-distribution with $k-1$ degrees of freedom.
%\end{itemize}

%---------------
%\begin{minipage}{\linewidth}
%\centering
%\resizebox{1.2\picwidth}{!}{\includegraphics{kruskal3chisqapprox}}
%\caption{Exact distribution of $H$ for $k=3$ and $n_1=n_2=n_3=3$.}
%\end{minipage}
%---------------


%\begin{itemize}
%\item
%In ANOVA, the residual sum of squares is defined by
%$$
%\SSE = \sum_{i=1}^k \sum_{j=1}^{n_i} (R_{ij}-\bar{R}_i)^2
%$$
%and the test statistic is the ratio,
%$$
%F = \frac{\SSM/(k-1)}{\SSE/(n-k)} 
%$$
%which has $F(k-1,n-k)$-distribution.
%\item 
%We do not need to compute $\SSE$ for the Kruskal-Wallis test.
%\end{itemize}
%
%Substituting for $\SSM$ and $\SST$,
%$$
%H = \frac{12}{n(n+1)}\sum_{i=1}^k n_i\bar{R}_i^2 - 3(n+1) 
%$$
%
%This is sometimes written as
%$$
%H = \frac{12}{n(n+1)}\sum_{i=1}^k \frac{T_i^2}{n_i} - 3(n+1) 
%$$
%where $T_i$ is the \emph{rank sum} of the $i$th group.
%

%%============================= 
%\subsection{Post hoc tests}
%%============================= 
%
%If the null hypothesis $H_0:\eta_1=\ldots=\eta_k$ is rejected by the Kruskal-Wallis test, we should
%\begin{itemize}
%\item
%perform post-hoc pairwise comparisons using \emph{Mann-Whitney} tests, and 
%\item 
%take steps to counter inflation of the Type I error rate (Bonferroni \textit{et al.}). 
%\end{itemize}


%%============================= 
%\subsection{Reporting Results}
%%============================= 
%For the Kruskal–Wallis test, we need to report
%\begin{itemize}
%\item the test statistic $H$,
%\item its degrees of freedom, and
%\item its  significance ($p$-value).
%\end{itemize}



\section*{Appendix: Sampling without replacement}
% lemma: mean and variance of sample mean (without replacement)
\begin{lemma}%[Sampling without replacement]
Let $X_1,X_2,\ldots,X_n$ be a random sample chosen uniformly at random \emph{without replacement} from a finite population $\{x_1,x_2,\ldots,x_N\}$. Then
\[
\expe(\bar{X}) = \mu \text{\quad and\quad} \var(\bar{X}) = \frac{\sigma^2}{n}\left(\frac{N-n}{N-1}\right).
\]
where $\mu$ and $\sigma^2$ are the common mean and variance of the $X_i$,
\[
\mu = \frac{1}{N}\sum_{j=1}^N x_j \text{\quad and\quad} \sigma^2 = \frac{1}{N}\sum_{j=1}^N (x_j-\mu)^2.
\]
%The mean and variance of the sample mean $\bar{X}=\frac{1}{n}\sum_{i=1}{n}X_i$ are
\end{lemma}

%\begin{remark}
%\bit
%%\it The term $\displaystyle\frac{N-n}{N-1}$ is called the \emph{finite population correction} (FPC) factor.
%\it The term $(N-n)/(N-1)$ is called the \emph{finite population correction} (FPC) factor.
%\eit
%\end{remark}

\begin{proof}
Let $I_j$ be the indicator variable of the event that one of the $X_i$ takes the value $x_j$:
\[
I_j = \left\{\begin{array}{ll}
	1	& \text{ if } x_j\in\{X_1,X_2,\ldots,X_n\} \\
	0	& \text{ otherwise.}
\end{array}\right.
\]
Then $I_j\sim\text{Bernoulli}(n/N)$, so $\expe(I_j)=n/N$ and $\var(I_j)= (n/N)(1-n/N)$
\par
The sample mean can be written as
\[
\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i = \frac{1}{n}\sum_{j=1}^N x_j I_j.
\]
%\bit
%\it $I_j\sim\text{Bernoulli}(n/N)$, so $\expe(I_j)=n/N$ and $\var(I_j)= (n/N)(1-n/N)$.
%\it $\sum_{i=1}^n X_i = \sum_{j=1}^N x_j I_j$.
%\eit

Hence the expected value of the sample mean is
\[
\expe(\bar{X}) 
	= \expe\left(\frac{1}{n}\sum_{j=1}^N x_jI_j\right)
	= \frac{1}{n}\sum_{j=1}^N x_j\expe(I_j) 
	= \frac{1}{N}\sum_{j=1}^N x_j
	= \mu.
\]

Because we are sampling without replacement, the $I_j$ are \emph{not} independent, so
%\begin{align*}
%\var(\bar{X}) 
%	& = \var\left(\frac{1}{n}\sum_{j=1}{N}x_jI_j\right) \\
%	& = \frac{1}{n^2}\var\big[\sum_{j=1}{N} x_jI_j\big] \\
%	& = \frac{1}{n^2}\big[\sum_{j=1}{N} x_j^2\var(I_j) + \sum_{j=1}^N\sum_{k\neq j}\cov(I_j,I_k)\big].
%\end{align*}
%\[
%\var(\bar{X}) 
%	= \frac{1}{n^2}\var\left(\sum_{j=1}^N x_jI_j\right)
%	= \frac{1}{n^2}\left(\sum_{j=1}^N x_j^2\var(I_j) + \sum_{j=1}^N\sum_{k=1\atop k\neq j}^N\cov(I_j,I_k)\right).
%\]
%
\[
\var(\bar{X}) 
	= \frac{1}{n^2}\var\left(\sum_{j=1}^N x_jI_j\right)
%	= \frac{1}{n^2}\left(\sum_{j=1}^N x_j^2\var(I_j) + \sum_{\stackss{j,k=1}{k\neq j}}^N x_kx_k\cov(I_j,I_k)\right).
	= \frac{1}{n^2}\left(\sum_{j=1}^N x_j^2\var(I_j) + \sum_{j=1}^N\sum_{k\neq j} x_j x_k\cov(I_j,I_k)\right).
\]




Now, $I_j I_k = 1$ if and only if $I_j=1$ and $I_k=1$ (and zero otherwise), and
\[
\prob(I_j=1,I_k=1) = \prob(I_j=1)\prob(I_k=1|I_j=1) = \frac{n}{N}\left(\frac{n-1}{N-1}\right).
\]
Hence $I_jI_k\sim\text{Bernoulli}\big[n(n-1)/N(N-1)\big]$, so
\begin{align*}
\cov(I_j,I_k)
	= \expe(I_jI_k)-\expe(I_j)\expe(I_k)
	= \frac{n(n-1)}{N(N-1)} - \frac{n^2}{N^2} 
	= -\left(\frac{n}{N}\right)\left(1-\frac{n}{N}\right)\frac{1}{N-1}.
\end{align*}
%and using the fact that $\var(I_j)=(n/N)(1-n/N)$,
Thus, using the fact that $\var(I_j)=\displaystyle\left(\frac{n}{N}\right)\left(1-\frac{n}{N}\right)$,
\begin{align*}
\var(\bar{X}) 
	& = \frac{1}{n^2}\left(\frac{n}{N}\right)\left(1-\frac{n}{N}\right)
		\left(\sum_{j=1}^N x_j^2 + \frac{1}{N-1}\sum_{\stackss{j,k=1}{k\neq j}}^N x_jx_k\right)
\end{align*}
The last term is equal to $\sigma^2N^2/(N-1)$:
%\begin{align*}
%N\sigma^2 
%	& = \sum_{j=1}^n x_j^2 - N\mu^2 \\
%	& = \sum_{j=1}^n x_j^2 - \frac{1}{N}\sum_{j=1}^N\sum_{k=1}^N x_jx_k \\
%%	& = \sum_{j=1}^n x_j^2 - \frac{1}{N}\sum_{j=1}^N\left(x_j^2 + \sum_{k\neq j} x_jx_k\right) \\
%	& = \left(1-\frac{1}{N}\right)\sum_{j=1}^n x_j^2 - \frac{1}{N}\sum_{j=1}^N\sum_{k\neq j} x_jx_k \\
%	& = \frac{N-1}{N}\left[\sum_{j=1}^n x_j^2 - \frac{1}{N}\sum_{j=1}^N\sum_{k\neq j} x_jx_k\right]
%\end{align*}
\begin{align*}
\sigma^2 
	= \frac{1}{N}\sum_{j=1}^n x_j^2 - \mu^2 \ 
	& = \frac{1}{N}\sum_{j=1}^n x_j^2 - \frac{1}{N^2}\sum_{j=1}^N\sum_{k=1}^N x_jx_k \\
%	& = \frac{1}{N}\sum_{j=1}^n x_j^2 - \frac{1}{N^2}\sum_{j=1}^N\left(x_j^2 + \sum_{k\neq j} x_jx_k\right) \\
	& = \frac{1}{N}\left(1-\frac{1}{N}\right)\sum_{j=1}^n x_j^2 - \frac{1}{N^2}\sum_{j=1}^N\sum_{k\neq j} x_jx_k \\
	& = \frac{N-1}{N^2}\left[\sum_{j=1}^n x_j^2 - \frac{1}{N-1}\sum_{j=1}^N\sum_{k\neq j} x_jx_k\right]
\end{align*}
Hence,
\[
\var(\bar{X}) 
	= \frac{1}{n^2}\left(\frac{n}{N}\right)\left(1-\frac{n}{N}\right)\left(\frac{N^2}{N-1}\right)\sigma^2
	= \frac{\sigma^2}{n}\left(\frac{N-n}{N-1}\right),
\]
as required.
\end{proof}

%======================================================================
\stopcontents[chapters]
\endinput
%======================================================================
