% !TEX root = main.tex
%----------------------------------------------------------------------
\chapter{The Bivariate Normal Distribution}\label{chap:bivariate_normal}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\section{Bivariate transformations}
%----------------------------------------------------------------------

% definition
\begin{definition}\label{def:jacobian}
Let $h:\R^2\to\R^2$ and let $(u,v) = h(x,y)$. The \emph{Jacobian determinant} of the transformation $h$ is the determinant of its $2\times 2$ matrix of partial derivatives:
\[
J = \begin{vmatrix}
\displaystyle\frac{\partial u}{\partial x} & \displaystyle\frac{\partial u}{\partial y} \\[2ex]
\displaystyle\frac{\partial v}{\partial x} & \displaystyle\frac{\partial v}{\partial y} \\[2ex]
\end{vmatrix}
\]
\end{definition}

% theorem: continuous
\begin{theorem}\label{thm:bivariate_transformation_pdf}
Let $U$ and $V$ be jointly continuous random variables, let $f_{U,V}$ be their joint PDF, let $g:\R^2\to\R^2$ be an injective transform over the support of $f_{U,V}$ and let $(X,Y) = g(U,V)$. Then the joint PDF of $X$ and $Y$ is given by
\[
f_{X,Y}(x,y) = |J| f_{U,V}\big[ g^{-1}(x,y) \big] 
\quad\text{where}\quad
J = 
\begin{vmatrix}
\displaystyle\frac{\partial u}{\partial x} & \displaystyle\frac{\partial u}{\partial y} \\[2ex]
\displaystyle\frac{\partial v}{\partial x} & \displaystyle\frac{\partial v}{\partial y} \\[2ex]
\end{vmatrix}
\quad\text{with}\quad
(u,v) = g^{-1}(x,y).
\]
\end{theorem}

% remark
\begin{remark}
The absolute value $|J|$ is a scale factor, which ensures that $f_{X,Y}(x,y)$ integrates to one.
\end{remark}

% example: jointly continuous
\begin{example}
Let $U$ and $V$ be continuous random variables, and let $X=U+V$ and $Y=U-V$. 
\ben
\it Find the joint PDF of $X$ and $Y$ in terms of the joint PDF of $U$ and $V$.
\it If $U,V\sim\text{Exponential}(1)$ are independent, find the joint PDF of $X$ and $Y$.
\een
\end{example}

\begin{solution}
\ben

\it % <<< (i)
\bit
\it The transformation $g:\R^2\to\R^2$ is defined by $g(u,v) = (u+v,u-v)$. 
\it To compute the inverse transformation $g^{-1}:\R^2\to\R^2$, we solve the equations
\[
x=u+v \quad\text{and}\quad y=u-v.
\]
\it This yields $u = \frac{1}{2}(x+y)$ and $v = \frac{1}{2}(x-y)$.
\it Thus the inverse transformation is 
\[
(u,v) = g^{-1}(x,y) = \left[\frac{1}{2}(x+y),\frac{1}{2}(x-y)\right].
\]
\eit
The Jacobian determinant is given by
\[
J = 
\begin{vmatrix}
\displaystyle\frac{\partial u}{\partial x} & \displaystyle\frac{\partial u}{\partial y} \\[2ex]
\displaystyle\frac{\partial v}{\partial x} & \displaystyle\frac{\partial v}{\partial y} \\
\end{vmatrix}
=
\begin{vmatrix}
1/2	& 1/2 \\
1/2 & -1/2 \\
\end{vmatrix}
= -\frac{1}{4} - \frac{1}{4} = -\frac{1}{2}.
\]
Hence the joint PDF of $X$ and $Y$ is
\begin{align*}
f_{X,Y}(x,y)
	& = |J|f_{U,V}(u,v) \\
	& = \left|-\frac{1}{2}\right| f_{U,V}\left[\frac{1}{2}(x+y),\frac{1}{2}(x-y)\right] \\
	& = \frac{1}{2}f_{U,V}\left[\frac{1}{2}(x+y),\frac{1}{2}(x-y)\right].
\end{align*}



\it % <<< (ii)
Let $U$ and $V$ be independent with $U,V\sim\text{Exponential}(1)$.

By independence, the joint PDF of $U$ and $V$ is 
\[
f_{U,V}(u,v) 
	= \begin{cases}
		e^{-(u+v)}		& u,v > 0 \\
		0				& \text{otherwise.}
\end{cases}
\]
To compute the support of $f_{X,Y}$, since $u>0$ and $v>0$ we have $x>0$, so
\bit
\it $\min(y) = \min(u-v) = -x$ (which occurs when $u=0$ and $v=x$), and
\it $\max(y) = \max(u-v) = x$ (which occurs when $u=x$ and $v=0$).
\eit
Thus, substituting for $u+v = \frac{1}{2}(x+y) + \frac{1}{2}(x-y) = x$, we obtain
\[
f_{X,Y}(x,y) = \begin{cases}
	\frac{1}{2}e^{-x}	& \text{for } x > 0 \text{ and } -x < y < x, \\
	0					& \text{otherwise.}
\end{cases}
\]
\een
\end{solution}


%----------------------------------------------------------------------
\section{The bivariate normal distribution}
%----------------------------------------------------------------------

%%----------------------------------------------------------------------
%\subsection{The sum of two random variables}
%%----------------------------------------------------------------------
%% theorem
%\begin{theorem}\label{thm:convolution}
%Let $X$ and $Y$ be jointly continuous random variables, and let $f_{X,Y}$ be their joint PDF. The random variable $X+Y$ has the following PDF:
%\[
%f_{X+Y}(t) 
%	= \int_{-\infty}^{\infty} f_{X,Y}(x,t-x)\,dx
%	= \int_{-\infty}^{\infty} f_{X,Y}(t-y,y)\,dy.
%\]
%\end{theorem}
%
%\begin{proof}
%Let $A = \{(x,y):x+y\leq z\}\subset\R^2$. Then
%\[
%\prob(X+Y\leq z)
%	= \iint_A f(x,y)\,dxdy
%	= \int_{x=-\infty}^{\infty} \int_{y=-\infty}^{z-x}f_{X,Y}(x,y)\,dy\,dx
%\]
%We change the variable of integration (in the inner integral), making the substitution $y=t-x$:
%\begin{align*}
%F_{X+Y}(z) = \prob(X+Y\leq z)
%	& = \int_{x=-\infty}^{\infty} \int_{t=-\infty}^{z}f_{X,Y}(x,t-x)\,dt\,dx \\
%	& = \int_{t=-\infty}^{z} \int_{x=-\infty}^{\infty}f_{X,Y}(x,t-x)\,dx\,dt
%\end{align*}
%where the final equality follows by reversing the order of integration. Thus the PDF of $X+Y$ is 
%\[
%f_{X+Y}(t) = \int_{-\infty}^{\infty} f_{X,Y}(x,t-x)\,dx \qquad\text{as required.}
%\]
%\end{proof}
%
%% corollary
%\begin{corollary}\label{cor:convolution_independent}
%If $X$ and $Y$ are independent, the PDF of $X+Y$ is the \emph{convolution} of the marginal PDFs:
%\[
%f_{X+Y}(t) 
%	= \int_{-\infty}^{\infty} f_X(x)f_Y(t-x)\,dx
%	= \int_{-\infty}^{\infty} f_X(t-y)f_Y(y)\,dy.
%\]
%\end{corollary}

%\begin{remark}
%The function $f_{X+Y}$ is called the \emph{convolution} of $X$ and $Y$, and is often written as $f_{X+Y}=f_X\ast f_Y$.
%\end{remark}

%%----------------------------------------------------------------------
%\subsection{The sum of two standard normal variables}
%%----------------------------------------------------------------------
%% lemma: sums of normal random variables}
%\begin{lemma}\label{lem:sum_of_standard_normal_variables}
%If $U,V\sim N(0,1)$ are independent, then $U+V\sim N(0,2)$.
%\end{lemma}
%
%\begin{proof}
%Since $U$ and $V$ are independent, their joint PDF is
%\[
%f(u,v)=f_U(u)f_V(v) = \frac{1}{2\pi}\exp\left(-\frac{1}{2}(u^2 + v^2)\right) \qquad  u,v\in\R.
%\]
%
%Let $W=U+V$. Since $U$ and $V$ are independent, by Corollary~\ref{cor:convolution_independent} we have
%\begin{align*}
%f_W(w) 
%	& = \int_{-\infty}^{\infty} f_U(u)f_V(w-u)\,du \\
%	& = \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp\left[-\frac{1}{2}\left(u^2 + (w-u)^2\right)\right]\,du \\
%	& = \frac{1}{2\pi} e^{-\frac{1}{4}w^2} \int_{-\infty}^{\infty} \exp\left[-\left(u-\frac{w}{2}\right)^2\right]\,du
%\end{align*}
%We change the variable of integration, by making the substitution $t = \displaystyle\sqrt{2}\left(u-\frac{w}{2}\right)$:
%\[
%f_W(w) 
%	= \frac{1}{2\sqrt{\pi}} e^{-\frac{1}{4}w^2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}w^2}\,dv
%	= \frac{1}{2\sqrt{\pi}} e^{-\frac{w^2}{4}},
%\]
%which is the PDF of the $N(0,2)$ distribution
%\end{proof}
%
%Lemma~\ref{lem:sum_of_standard_normal_variables} is a special case of the following theorem.
%% theorem

%%----------------------------------------------------------------------
%\section{The standard bivariate normal distributrion} 
%%----------------------------------------------------------------------

\begin{theorem}\label{thm:sum_of_normal_variables}
if $X\sim N(\mu_1,\sigma_1^2)$ and $Y\sim N(\mu_2,\sigma_2^2)$ are independent, then
\[
X+Y\sim N(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2).
\]
\end{theorem}
\proofomitted

%The following corollary will be useful later on:
% corollary
\begin{corollary}\label{cor:lin_comb_std_normal}
If $U,V\sim N(0,1)$ are independent, then $aU+bV\sim N(0,a^2+b^2)$ for all $a,b\in\R$.
\end{corollary}
%If $U$ and $V$ are independent standard normal variables, their joint PDF of is
%
%\[
%f(u,v)=f_U(u)f_V(v) = \frac{1}{2\pi}\exp\left(-\frac{1}{2}(u^2 + v^2)\right) \qquad  u,v\in\R.
%\]

\newpage

% definition: standard bivariate normal
\begin{definition}\label{def:standard_bivariate_normal}
A pair of random variables $U$ and $V$ have the \emph{standard bivariate normal distribution} if their joint PDF $f:\R^2\to[0,\infty)$ can be written as
\[
f_{U,V}(u,v) = \frac{1}{2\pi\sqrt{1-\rho^2}}\exp\left(-\frac{1}{2(1-\rho^2)}\big(u^2 - 2	\rho uv + v^2\big)\right)
\]
where $\rho$ is a constant satisfying $-1 < \rho < 1$.
\end{definition}

% definition: bivariate normal
\begin{definition}\label{def:bivariate_normal}
A pair of random variables $X$ and $Y$ are said to have \emph{bivariate normal distribution} with means $\mu_1$ and $\mu_2$, variances $\sigma_1^2$ and $\sigma_2^2$ and correlation $\rho$, if their joint PDF can be written as
%\small
\[
f_{X,Y}(x,y)
	= \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}
		\exp\left(-\frac{1}{2(1-\rho^2)}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2 
			-2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right) 
				+\left(\frac{y-\mu_2}{\sigma_2}\right)^2 \right]\right)
\]
\normalsize
\end{definition}

% technical result
The following lemma can be used to derive many properties of the bivariate normal distribution.
\begin{lemma}\label{lem:trick}
Let $U,V\sim N(0,1)$ be independent, let $\rho\in(-1,+1)$. Then the random variables
\begin{align*}
X & = \mu_1 + \sigma_1 U, \\
Y & = \mu_2 + \sigma_2\big(\rho U +\sqrt{1-\rho^2}V\big)
\end{align*}
have bivariate normal distribution with means $\mu_1$ and $\mu_2$, variances $\sigma_1^2$ and $\sigma_2^2$, and correlation $\rho$.
\end{lemma}

% proof
\begin{proof}
To find the joint PDF of $X$ and $Y$, let $g(u,v)$ denote the transformation:
\[
g(u,v) = \big[\mu_1 + \sigma_1 u, \mu_2 + \sigma_2(\rho u +\sqrt{1-\rho^2}v)\big].
\]
The inverse transformation is
\[
g^{-1}(x,y) 
	= \left(\frac{x-\mu_1}{\sigma_1}, \frac{1}{\sqrt{1-\rho^2}}
  \left[\left(\frac{y-\mu_2}{\sigma_2}\right) -\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\right]\right)
\]
The joint PDF of $X$ and $Y$ is $f_{X,Y}(x,y) = |J|f_{U,V}(u,v)$, where $J$ is the Jacobian determinant of the inverse transformation:
\[
J=\begin{vmatrix}
\displaystyle\frac{\partial u}{\partial x}  & \displaystyle\frac{\partial u}{\partial y}  \\[2ex]
\displaystyle\frac{\partial v}{\partial x}  & \displaystyle\frac{\partial v}{\partial y} 
\end{vmatrix}
=
\begin{vmatrix}
\displaystyle\frac{1}{\sigma_1}  		& \displaystyle0 \\[2ex]
\displaystyle\frac{1}{\rho\sigma_1}	& \displaystyle\frac{1}{\sigma_2\sqrt{1-\rho^2}} 
\end{vmatrix}
=
\frac{1}{\sigma_1\sigma_2\sqrt{1-\rho^2}}
\]

Because $U$ and $V$ are independent,
\[
f_{U,V}(u,v)=f_U(u)f_V(v) = \frac{1}{2\pi}\exp\left(-\frac{1}{2}(u^2 + v^2)\right) \qquad  u,v\in\R.
\]
and since
\begin{align*}
u^2 +v^2 
	&  = \left(\frac{x-\mu_1}{\sigma_1}\right)^2 
			+ \frac{1}{1-\rho^2}\left[ 
				\left(\frac{y-\mu_2}{\sigma_2}\right)^2 
					- 2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_{2}}\right) 
						+\rho^2\left(\frac{x-\mu_1}{\sigma_1}\right)^2 \right]  \\
	& =  \frac{1}{1-\rho^2}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2 
				-2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right) 
					+\left(\frac{y-\mu_2}{\sigma_2}\right)^2\right]
\end{align*}
it follows that
%\small
\[
f_{X,Y}(x,y)
	= \frac{1}{2\pi\sigma_1\sigma_1\sqrt{1-\rho^2}}
		\exp\left(\frac{-1}{2(1-\rho^2)}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2 
			-2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right) 
				+\left(\frac{y-\mu_2}{\sigma_2}\right)^2 \right]\right)
\]
\normalsize
as required.
\end{proof}

The following theorem shows that if $X$ and $Y$ have bivariate normal distribution, then any linear combination of $X$ and $Y$ is normally distributed.
% theorem: linear combination
\begin{theorem}\label{thm:lin_comb_bivar_normal}
Let $X$ and $Y$ have bivariate normal distribution with means $\mu_1$ and $\mu_2$, variances $\sigma_1^2$ and $\sigma_2^2$, and correlation $\rho$. Then
%If $X\sim N(\mu_1,\sigma_1^2)$ and $Y\sim N(\mu_2,\sigma_2^2)$ then
\[
aX + bY \sim N\big(a\mu_1 + b\mu_2, a^2\sigma_1^2 + 2ab\sigma_1\sigma_2\rho + b^2\sigma_2^2\big)
\]
\end{theorem}
\begin{proof}
Let $Z=aX+bY$, let $U$ and $V$ be independent standard normal random variables, and let
\begin{align*}
X' & = \mu_1 + \sigma_1 U \\
Y' & = \mu_2 + \sigma_2\big(\rho U +\sqrt{1-\rho^2}V\big)
\end{align*}

By Lemma~\ref{lem:trick}, $X$ and $Y$ have the same joint distribution as $X'$ and $Y'$, so $Z=aX+bY$ has the same distribution as 
\[
Z' = aX'+bY' = (a\mu_1 + b\mu_2) + (a\sigma_1 + b\sigma_2\rho)U + b\sigma_2\sqrt{1-\rho^2} V
\]

Because $U,V\sim N(0,1)$ are independent, it follows by Corollary~\ref{cor:lin_comb_std_normal} that
\[
Z' \sim N\left(a\mu_1 + b\mu_2, a^2\sigma_1^2 + 2ab\sigma_1\sigma_2\rho + b^2\sigma_2^2\right),
\]
so $Z=aX+bY$ has normal distribution, as required.
\end{proof}

%----------------------------------------------------------------------
\section{Properties of the bivariate normal distribution} 
%----------------------------------------------------------------------

% theorem: properties
\begin{theorem}
Let $X$ and $Y$ have bivariate normal distribution with means $\mu_1$ and $\mu_2$, variances $\sigma_1^2$ and $\sigma_2^2$, and correlation $\rho$. Then
\ben
\it $X\sim N(\mu_1,\sigma_1^2)$ and $Y\sim N(\mu_2,\sigma_2^2)$,
\it $\rho$ is the correlation coefficient of $X$ and $Y$, and
\it $X$ and $Y$ are independent if and only if $\rho=0$.
\een
\end{theorem}

% proof
\begin{proof}
%By Lemma~\ref{lem:trick}, let us write
Let $U,V\sim N(0,1)$ and define
\begin{align*}
X & = \mu_1 + \sigma_1 U \\
Y & = \mu_2 + \sigma_2\big(\rho U +\sqrt{1-\rho^2}V\big)
\end{align*}


\ben
\it % (i)
In the proof of Theorem~\ref{thm:lin_comb_bivar_normal}:
\bit 
\it taking $a=1$ and $b=0$ yields $X\sim N(\mu_1,\sigma_1^2)$, and
\it taking $a=0$ and $b=1$ yields $Y\sim N(\mu_2,\sigma_2^2)$.
\eit
\it % (ii)
Using the fact that $\cov(aX+b,cY+d)=ac\cov(X,Y)$ for all $a,b,c,d\in\R$,
\begin{align*}
\cov(X,Y)
	& = \cov\big[\mu_1 + \sigma_1 U, \mu_2 + \sigma_2(\rho U + \sqrt{1-\rho^2}V)\big] \\
	& = \sigma_1\sigma_2\cov(U, \rho U + \sqrt{1-\rho^2}V) \\
	& = \sigma_1\sigma_2\big[\rho\expe(U^2) + \sqrt{1-\rho^2}\expe(UV)\big] \\
	& = \sigma_1\sigma_2\rho.
\end{align*}
Thus $\rho = \displaystyle\frac{\cov(X)}{\sqrt{\var(X)}\sqrt{\var(Y)}}$ as required.


\it % (iii)
If $X$ and $Y$ are independent, they are uncorrelated. If $X$ and $Y$ are uncorrelated then $\rho=0$, so the joint PDF of $X$ and $Y$ satisfies
\begin{align*}
f_{X,Y}(x,y)
	& = \frac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\frac{1}{2}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2 + \left(\frac{y-\mu_2}{\sigma_2}\right)^2 \right]\right) \\
	& = \frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2\right)
			\times \frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{1}{2}\left(\frac{y-\mu_2}{\sigma_2}\right)^2\right) \\
	& = f_X(x)f_Y(y).		
\end{align*}
Because this holds for all $x,y\in\R$, it follows that $X$ and $Y$ are independent. 
\een
\end{proof}

%----------------------------------------------------------------------
\section{Conditional distributions} 
%----------------------------------------------------------------------

% theorem
\begin{theorem}
Let $X$ and $Y$ have bivariate normal distribution with means $\mu_1$ and $\mu_2$, variances $\sigma_1^2$ and $\sigma_2^2$, and correlation $\rho$.
Then the conditional distribution of $Y$ given $X=x$ is also normal, with conditional mean and variance given by
\begin{align*}
\expe(Y|X=x)	& = \mu_2 + \rho\left(\frac{\sigma_2}{\sigma_1}\right)(x - \mu_1), \\[2ex]
\var(Y|X=x) 	& = \sigma_2^2(1-\rho^2),
\end{align*}
and the conditional mean and variance of $Y$ given $X$ is 
\begin{align*}
\expe(Y|X)	& = \expe(Y) + \frac{\cov(X,Y)}{\var(X)}\big[X - \expe(X)\big], \\[2ex]
\var(Y|X) 	& = \var(Y)(1-\rho^2).
\end{align*}
\end{theorem}

% proof
\begin{proof}
Let $U,V\sim N(0,1)$ be independent, and define the random variables
\begin{align*}
X & = \mu_1 + \sigma_1 U, \\
Y & = \mu_2 + \sigma_2\big[\rho U +\sqrt{1-\rho^2}V\big] \\
	& = \mu_2 + \sigma_2\left[\rho \left(\frac{X-\mu_1}{\sigma_1}\right) +\sqrt{1-\rho^2}V\right].
\end{align*}
If $X$ is fixed at $x$, then $Y$ is a linear transformation of $V$, so the conditional distribution of $Y$ given that $X=x$ is a normal distribution. 
Furthermore, since $\expe(V)=0$ and $\var(V)=1$ we have
\begin{align*}
\expe(Y|X=x) 	& = \mu_2 + \rho\left(\frac{\sigma_2}{\sigma_1}\right)(x - \mu_1) \\[2ex]
\var(Y|X=x)	& = \sigma_2^2(1-\rho^2)
\end{align*}
as required.
\end{proof}

%% remarks
%\begin{remark}
%\bit
%\it
%Given $X=x$, the conditional mean is obtained by adjusting $\expe(Y)$ by an amount proportional to the difference between $X=x$ and its mean $\expe(X)$. The size of this adjustment is determined by (1) the size of $Y$ relative to $X$, expressed by the ratio $\sigma_2/\sigma_1$, and (2) the correlation coefficient $\rho$, which quantifies the linear dependence between $X$ and $Y$. Note that $\rho=0$ implies that $\expe(Y|X=x)=0$ for all $x$ (which is not surprising, given that uncorrelated normal variables are independent).
%\it 
%The conditional variance $\var(Y|X=x)$ quantifies the variability in $Y$ that is \emph{not} explained by the fact that $X$ takes the value $x$. The squared correlation coefficient $\rho^2$ thus quantifies the proportion of the overall variance $\var(Y)$ accounted for by the fact that $X=x$. 
%\it
%This idea of `explained' and `unexplained' variance is apparent in the law of total variance:
%\[
%\var(Y) = \expe\big[\var(Y|X)\big] + \var\big[\expe(Y|X)\big] 
%\]
%	\bit
%	\it $\var(Y)$ is the total variance,
%	\it $\expe\big[\var(Y|X)\big]$ is the variance explained by $X$, and 
%	\it $\var\big[\expe(Y|X)\big]$ is the variance not explained by $X$. 
%	\eit
%\eit
%\end{remark}

%%----------------------------------------------------------------------
%\section{Mulivariate transformations}
%%----------------------------------------------------------------------
%
%% definition
%\begin{definition}\label{def:jacobian}
%Let $h:\R^n\to\R^n$ be a transformation of $n$ variables, and let $(x_1,x_2,\ldots,x_n) = h(y_1,y_2,\ldots,y_n)$. The \emph{Jacobian} (or \emph{Jacobian determinant}) of the transformation $h$ is the determinant of its $n\times n$ matrix of partial derivatives:
%\[
%J = 
%\begin{vmatrix}
%\displaystyle\frac{\partial x_1}{\partial y_1} & \displaystyle\frac{\partial x_1}{\partial y_2} & \cdots &  \displaystyle\frac{\partial x_1}{\partial y_n}\\[2ex]
%\displaystyle\frac{\partial x_2}{\partial y_1} & \displaystyle\frac{\partial x_2}{\partial y_2} & \cdots &  \displaystyle\frac{\partial x_2}{\partial y_n}\\[2ex]
%\vdots & \vdots & \ddots & \vdots \\[2ex]
%\displaystyle\frac{\partial x_n}{\partial y_1} & \displaystyle\frac{\partial x_n}{\partial y_2} & \cdots &  \displaystyle\frac{\partial x_n}{\partial y_n}\\[2ex]
%\end{vmatrix}
%\]
%\end{definition}
%
%% theorem: continuous
%\begin{theorem}\label{thm:transf_injective_joint_continuous}
%Let $X=(X_1,X_2,\ldots,X_n)$ be a vector of continuous random variables, let $f_X(x_1,x_2\ldots,x_n)$ be their joint PDF, let $g:\R^n\to\R^n$ be an injective transformation, and let $Y=(Y_1,Y_2,\ldots,Y_n)$ be the continuous random vector defined by $Y=g(X)$, i.e.\
%\[
%(Y_1,Y_2,\ldots,Y_n) = g(X_1,X_2,\ldots,X_n)
%\]
%If the Jacobian of the inverse transformation $g^{-1}$ is continuous and non-zero over the range of the transformation, the joint PDF of $Y$ is
%\[
%f_Y(y_1,y_2,\ldots,y_n) = |J| f_X(x_1,x_2,\ldots,x_n)
%\]
%where 
%\[
%(x_1,x_2,\ldots,x_n)=g^{-1}(y_1,y_2,\ldots,y_n)
%\]
%is the solution of $(y_1,y_2,\ldots,y_n)=g(x_1,\ldots,x_n)$.
%\end{theorem}
%
%% remark
%\begin{remark}
%The scale factor $|J|$ ensures that $f_Y(y_1,y_2,\ldots,y_n)$ integrates to one.
%\end{remark}
%
%
%%==================================================================================================
%\section{The multivariate normal distribution} 
%%==================================================================================================
%The bivariate normal distribution can be formulated using matrix notation. Let
%\[
%\mathbf{Z}		= \begin{pmatrix} X \\ Y \end{pmatrix},\quad
%\mathbf{\mu}	= \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}\quad\text{and}\quad
%\Sigma			= \begin{pmatrix} \sigma^2_1 & \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 & \sigma^2_2 \end{pmatrix}
%\]
%
%The matrix $\Sigma$ is called the \emph{variance-covariance} matrix.
%\bit
%\it The determinant of $\Sigma$ is 
%\[
%|\Sigma| = (1-\rho^2)\sigma^2_1\sigma^2_2.
%\]
%\it The inverse of $\Sigma$ is
%\[
%\Sigma^{-1} = \frac{1}{(1-\rho^2)\sigma^2_1\sigma^2_2}
%\begin{pmatrix} 
%	\sigma^2_2  				& -\rho\sigma_1\sigma_2 \\
%	-\rho\sigma_1\sigma_2	& \sigma^2_1
%\end{pmatrix}.
%\]
%\eit
%
%Let $\mathbf{z} = (x,y)^T$ and consider the quadratic form 
%\begin{align*}
%\big(\mathbf{z}-\mathbf{\mu})^T \Sigma^{-1}(\mathbf{z}-\mathbf{\mu}\big) 
%	& = 	\frac{1}{(1-\rho^2)\sigma^2_1\sigma^2_2}\ 
%			\begin{pmatrix} x-\mu_1 \\ y-\mu_2	\end{pmatrix}^T
%			\begin{pmatrix} \sigma^2_2 & -\rho\sigma_1\sigma_2 \\ -\rho\sigma_1\sigma_2 & \sigma^2_1 \end{pmatrix}
%			\begin{pmatrix} x-\mu_1 \\ y-\mu_2	\end{pmatrix} \\
%	& = \frac{1}{(1-\rho^2)}\left[\left(\frac{x-\mu_1}{\sigma_1}\right)^2
%			 -2\rho\left(\frac{x-\mu_1}{\sigma_1}\right)\left(\frac{y-\mu_2}{\sigma_2}\right) 
%			 	+\left(\frac{y-\mu_2}{\sigma_2}\right)^2\right]
%\end{align*}
%
%The PDF $f(x,y)$ of the bivariate normal distribution can therefore be written as
%\[
%f(x,y) = \frac{1}{2\pi|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{z}-\mathbf{\mu})\right).
%\]		
%
%Using this notation, we define the \emph{multivariate normal distribution} as follows.
%
%
%
%% definition: multivariate normal
%\begin{definition}
%A random vector $\mathbf{Z}=(X_1,X_2,\ldots,X_n)^T$ is said to have \emph{multivariate normal distribution} with mean vector $\mathbf{\mu}= (\mu_1,\mu_2,\ldots,\mu_n)^T$ and variance-covariance matrix
%\[
%\Sigma			= \begin{pmatrix} 
%	\sigma^2_1		 			& \rho_{12}\sigma_1\sigma_2  	& \ldots & \rho_{1n}\sigma_1\sigma_n \\ 
%	\rho_{21}\sigma_1\sigma_2 	& \sigma^2_2					& \ldots & \rho_{2n}\sigma_2\sigma_n \\
%	\vdots						& \vdots						& \ddots & \vdots \\
%	\rho_{n1}\sigma_1\sigma_n		 	& \rho_{n2}\sigma_n\sigma_2	& \ldots & \sigma^2_n \\
%	\end{pmatrix}
%\]
%if its joint density function can be written as
%\[
%f(\mathbf{z}) = \frac{1}{2\pi|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{z}-\mathbf{\mu})\right)
%\]		
%\end{definition}
%
%where $\mu_i = \expe(X_i)$, $\sigma_i^2 = \var(X_i)$ and $\rho_{ij} = \displaystyle\frac{\expe(X_iX_j)}{\sigma_i\sigma_j}$.
%


%----------------------------------------------------------------------
\newpage
\section{Exercises}
\input{ex20_bivariate_normal}
%----------------------------------------------------------------------

%======================================================================
\endinput
%======================================================================
